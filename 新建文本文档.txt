明白了densnet这些的优点
1、减轻了vanishing-gradient（梯度消失） 
2、加强了feature的传递 
3、更有效地利用了feature 
4、一定程度上较少了参数数量

感觉和之前学习的卷积再池化好像。每一层的输出相当于下一层的输入，更加好的学习上一层的东西，再得到下一层的结果。最后再softmax。不知道是否有一些完善的框架呢?自己虽然理解这些，但是写得不够好.
